\section{Introduction} \label{sec:intro} 

Current LSS analyses rely on analytic models (i.e. perturbation theory) to calculate 
the likelihood combined with a Bayesian parameter inference framework to derive 
posterior probability distrubtions of cosmological parameters. While much progress 
has been made to extend perturbation theory to smaller more nonlinear scales, 
perturbation theory eventually breaks down. This is a shame because there's significant 
constraining power on small scales. 

N-body simulations don't have the same issues on small scales. There's been a lot of 
progress in making fast simulations (e.g. fastpm, etc). In principle, we can exploit
the constraining power on small scales, beyond perturbation theory limits, by calculating 
the likelihood directly using simulations. However, even with their progress, simulations 
are expensive and in standard MCMC analyses require evaluating the likelihood $>100,000$ 
times. This makes a naive simulation based approach currently intractable. 

Emulation based approaches have been put forth to address this issue. These approaches 
exploit the accuracy of simulations on small scales while reducing the compute time 
necessary. In this approach an emulator is built using simulations evaluated at a set 
of parameters spanning the parameter space to be explored. Most recent works have 
utilized space-filling sampling methods such as latin hypercubes (citecite) and build
their emulators using Gaussian Processes (cite) or chaotic polynomial (cite). Yet even 
with such approaches the number of simulations is pretty large. Give numbers from Aemulus. 
    
One reason why we still need a lot of sims even for emulators is sample variance. Sample
variance is the statistical fluctations in our observables that come from using a finite 
size simulation. It directly contributes to the theoretical uncertainty of the emulator. 
Hence multiple realizations are necessary to beat down sample variance and get a less 
noisy estimate on the mean observable. 

Recently, \cite{pontzen2016} and \cite{angulo2016} proposed a method to suppress sample
variance using \emph{paired fixed simulations}. define in some concise way paired, fixed, 
and paired-fixed sims (see below). 

\beq
\delta(\bfi{x}) = \frac{\rho(\bfi{x}) - \bar{\rho}}{\bar{\rho}}
\eeq

\beq
\delta(\bfi{k}) = \frac{1}{(2\pi)^3} \int {\rm d}^3\bfi{x} e^{-i\bfi{k} \cdot \bfi{x}} \delta(\bfi{x}) = A e^{i\theta}
\eeq

For Gaussian random field, $\theta$ is uniformly sampled from $0$ to $2\pi$ 
and $A$ is sampled from Rayleigh distribution:
\beq
p(A) {\rm d}A = \frac{A}{\sigma^2} e^{-A^2/2\sigma^2} {\rm d}A
\eeq
where $\sigma^2 = V~P(k)/(16\pi^3)$. The mean of this distribution is 
\beq
\langle A\rangle = \int_0^\infty \frac{A^2}{\sigma^2} e^{-A^2/2\sigma^2} {\rm d}A = \sqrt{\frac{V\,P(k)}{32\pi^2}}.
\eeq
Also, 
\beq
\langle \delta(\bfi{k})\delta^*(\bfi{k}) \rangle = \langle A^2\rangle = \int_0^\infty \frac{A^3}{\sigma^2} e^{-A^2/2\sigma^2} {\rm d}A = \frac{V\,P(k)}{(2\pi)^3}.
\eeq

A paired Gaussian field is where you have two fields $\delta_1$ and $\delta_2$ 
where $\delta_2(k) = A e^{i(\theta + \pi)} = - \delta_1(k)$. 

A fixed field is when the amplitude is fixed, 
\beq
A = \sqrt{\frac{V~P(k)}{(2\pi)^3}},
\eeq
such that the power spectrum is the same. 

Paired fixed is when you do both. 


\cite{villaescusa-navarro2018a} and \cite{chuang2019} recently examined whether paired fixed simulations
introduce any bias for a variety of observables. list observables. In this work, we focus on whether paired-fixed 
simulations introduce any bias for the full real-space and redshift-space bispectrum using over $23000$ $N$-body
simulations of the \quij simulation suite. Furthermore, we examine whether these biases can propagate to parameter 
inference.  

